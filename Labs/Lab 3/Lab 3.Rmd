---
title: "STA2201 Lab #3"
author: "Timothy Jordan Regis"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, include=FALSE}
library(ggplot2)
library(kableExtra)
library(dplyr)
library(tidyverse)
```



# Q1

$$Y|\theta = \binom{n}{y} \theta^y(1 - \theta)^{n - y}$$

Log-Likelihood:

$$= log\binom{n}{y} + ylog(\theta) + (n - y)log(1 - \theta)$$

$$\frac{\partial }{\partial \theta} = 0:$$

$$0 =  \frac{y}{\theta} - \frac{n - y}{1 - \theta}$$

$$\frac{y}{\theta} = \frac{n - y}{1 - \theta}$$

$$y(1 - \theta) = (n - y)\theta$$

$$\hat\theta = \frac{y}{n} = \frac{118}{129}$$

Variance for CI:

$$Var[\hat\theta] = \frac{Var[y]}{n^2}$$

$$ = \frac{n\hat\theta(1 - \hat\theta)}{n^2}$$
$$ = \frac{\hat\theta(1 - \hat\theta)}{n}$$
$$ = \frac{\frac{118}{129}(1 - \frac{118}{129})}{129}$$

95% CI:

```{r}

y = 118
n = 129
theta_hat = y/n

var_theta_hat = (theta_hat*(1 - theta_hat))/n

# CI:

Upper = theta_hat + qnorm(0.975) * sqrt(var_theta_hat)
Lower = theta_hat - qnorm(0.975) * sqrt(var_theta_hat)

kable(data.frame(cbind(Lower, "Estimate" = theta_hat, Upper)), digits = 2)
```


# Q2

As we saw in lecture, a Beta(1, 1) distribution has a pdf of $p(\theta) = 1$, and the posterior distribution of a Beta(a, b) prior and Binomial(y, n, $\theta$) is distributed as:

$Beta(y + a, n - y + b)$

With expectation:

$\frac{a}{a + b}$

Thus, the posterior mean is:

$$E[P(\theta|y)] = \frac{y + 1}{y + 1 + n - y + 1} = \frac{y + 1}{n + 2}$$
```{r}
theta_hat2 = (y + 1)/(n + 2)

Upper2 = qbeta(0.975, shape1 = y + 1, shape2 = n - y + 1)
Lower2 = qbeta(0.025, shape1 = y + 1, shape2 = n - y + 1)
```

With the following 95% CI:
```{r}
kable(data.frame(cbind("Lower" = Lower2, "Estimate" = theta_hat2, "Upper" = Upper2)), digits = 2)
```


# Q3

With a Beta(10, 10) prior this assumes we have seen 9 successes and 9 failures in the data, and thus concentrates its distribution more around 0.5 than a Beta(1, 1) prior. Therefore this prior assumes we know considerably more information of the true value of theta as compared to the Beta(1, 1) prior

```{r}
# I wasn't sure if this was needed.
# 
# theta_hat3 = (y + 10)/(n + 20)
# 
# Upper2 = qbeta(0.975, shape1 = y + 10, shape2 = n - y + 10)
# Lower2 = qbeta(0.025, shape1 = y + 10, shape2 = n - y + 10)
# 
# kable(data.frame(cbind("Lower" = Lower3, "Estimate" = theta_hat3, "Upper" = Upper3)), digits = 2)
```

# Q4


```{r, fig.align='center', fig.width=8, fig.height=5}
vals = c()
theta = seq(0, 1, 0.001)

for(i in 1:length(theta)){
  vals[i] = dbinom(118, 129, theta[i])
}

hist(vals,
     xlab = "Likelihood",
     main = "Histogram of Likelihoods",
     col = "steelblue")

ggplot() + 
  xlim(0, 1) + 
  stat_function(aes(colour = 2),
                fun = dbinom,
                args = list(x = y, size = n)) +
  labs(title = "Likelihood") + 
  xlab("Theta") + 
  ylab("Likelihood") + 
  theme_minimal() + 
  theme(legend.position = "none")

# plot(y = vals, x = theta, type = "l",
#      ylab = "Density/Likelihood",
#      xlab = "Theta",
#      col = "red")




```

As we can see, the likelihood has a very high concentration around 0.9, which corresponds to the estimates of $\theta$ we generated in the previous questions. Moreover, on points below 0.8 this likelihood drops to 0, suggesting good confidence in the true value of $\theta$.


```{r, fig.align='center', fig.width=8, fig.height=5}
ggplot() + 
  xlim(0, 1) + 
  stat_function(aes(colour = "Beta(1, 1) Prior"), 
                fun = dbeta, 
                args = list(shape1 = 1, shape2 = 1)) +
  stat_function(aes(colour = "Beta(y + 1, n - y + 1) Post."), 
                fun = dbeta, 
                args = list(shape1 = y + 1, shape2 = n - y + 1)) +
  stat_function(aes(colour = "Beta(10, 10) Prior"), 
                fun = dbeta, 
                args = list(shape1 = 10, shape2 = 10)) +
  stat_function(aes(colour = "Beta(y + 10, n - y + 10) Post."), 
                fun = dbeta, 
                args = list(shape1 = y + 10, shape2 = n - y + 10)) + 
  labs(title="Priors & Posteriors") + 
  xlab("Theta") + 
  ylab("") + 
  scale_colour_discrete(name = "Line") + 
  theme_minimal()
```




As we can see, the Beta(1, 1) prior is extremely uninformative, as it is represented by the completely flat red line. On the other hand, the Beta(10, 10) prior is considerably more informative with its concentrated density around 0.5.
The posteriors, show a less noticeable difference, where the main concentration for the Beta(y + 10, n - y + 10) posterior occurs at just a slightly lower value of $\theta$ than the Beta(y + 1, n - y + 1) posterior, which again corresponds to the difference we see in the two estimates generated previously as without rounding these values would be 0.908 and 0.914 respectively.



# Q5



An uninformative prior we could use here could be any uniformly distributed prior around 0, such as Uniform(-10, 10) where it suggests equal assumptions for a player performing better or worse after the program, in turn leaving the likelihood's estimate fairly unchanged.

A more informative prior would be one with a high concentration above 0, such as Beta(20, 2) which suggests a much greater average improvement in success probability after the program.















